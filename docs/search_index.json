[["index.html", "Workshop material", " Workshop material In the following link you will find the R routines developed for the workshop. Descargar "],["sesión-1--census-and-satellite-information.html", "Chapter 1 Sesión 1- Census and satellite information ", " Chapter 1 Sesión 1- Census and satellite information "],["use-of-satellite-imagery-and-sae.html", "1.1 Use of Satellite Imagery and SAE", " 1.1 Use of Satellite Imagery and SAE One of the pioneering articles in small area estimation was the paper by Singh, R, et al. (2002), which addressed crop yield estimation for the tehsils (sub-administrative units) of Rohtak district in Haryana, India. Raster images represent the world through a set of contiguous equally spaced cells known as pixels. These images contain information like a geographic information system and a coordinate reference system. Images store an identifier, a value in each pixel (or a vector with different values), and each cell is associated with a color scale. Images can be obtained in raw and processed forms. The former contains only color layers, while the latter also contains values that have been processed in each cell (vegetation indices, light intensity, type of vegetation). Raw information can be used to train desired features (roads, crop types, forest/non-forest). Fortunately, in Google Earth Engine, we find many processed indicators associated with a pixel. These indicators can be aggregated at a geographical area level. "],["satellite-image-data-sources.html", "1.2 Satellite Image Data Sources", " 1.2 Satellite Image Data Sources Some of the main sources of satellite images include: USGS Earth Explorer Land Processes Distributed Active Archive Center (LP DAAC) NASA Earthdata Search Copernicus Open Access Hub AWS Public Dataset - Landsat However, most of these sources are centralized within Google Earth Engine, which allows searching for satellite image data sources. GEE can be managed through APIs in different programming languages: JavaScript (by default), Python, and R (rgee package). Here’s the translation: "],["google-earth-engine.html", "1.3 Google Earth Engine", " 1.3 Google Earth Engine Create an account at this link. Once logged in, you can search for datasets of interest: Night Lights Image Upon searching for the dataset, you can open a code editor provided by Google in JavaScript. Copy and paste the syntax provided by the dataset search to visualize the raster image and obtain statements allowing for the retrieval of the dataset of interest later in R. Syntax in JavaScript Sure, here’s the translation of the instructions: "],["installing-rgee.html", "1.4 Installing rgee", " 1.4 Installing rgee Download and install Anaconda or Conda from here. Open Anaconda Prompt and set up a working environment (Python environment JAM2023) using the following commands: conda env list conda create -n JAM2023 python=3.9 activate JAM2023 pip install google-api-python-client pip install earthengine-api pip install numpy List available Python environments in Anaconda Prompt: conda env list Once you’ve identified the path of the JAM2023 environment, set it in R (remember to change \\ to /). Install reticulate and rgee, load packages for spatial processing, and set up the working environment as follows: library(reticulate) # Connection with Python library(rgee) # Connection with Google Earth Engine library(sf) # Package for handling geographic data library(dplyr) # Package for data processing library(magrittr) rgee_environment_dir = &quot;C:/Users/gnieto/Anaconda3/envs/JAM2023/python.exe&quot; # Set up Python (Sometimes not detected and R needs to be restarted) reticulate::use_python(rgee_environment_dir, required=T) rgee::ee_install_set_pyenv(py_path = rgee_environment_dir, py_env = &quot;JAM2023&quot;) Sys.setenv(RETICULATE_PYTHON = rgee_environment_dir) Sys.setenv(EARTHENGINE_PYTHON = rgee_environment_dir) Once the environment is configured, you can initialize a Google Earth Engine session as follows: rgee::ee_Initialize(drive = T) Session started successfully Notes: Each session must be initialized with the command rgee::ee_Initialize(drive = T). JavaScript commands invoking methods with “.” are replaced by the dollar sign ($), for example: ee.ImageCollection().filterDate() # JavaScript ee$ImageCollection()$filterDate() # R ¡Claro! Aquí tienes la traducción de los pasos para descargar información satelital: 1.4.1 Downloading Satellite Information Step 1: Have the shapefiles ready. shape &lt;- read_sf(&quot;Shapefile/JAM2_cons.shp&quot;) plot(shape[&quot;geometry&quot;]) Shapefile Step 2: Select the image file you want to process, for example, night lights. lights &lt;- ee$ImageCollection(&quot;NOAA/DMSP-OLS/NIGHTTIME_LIGHTS&quot;) %&gt;% ee$ImageCollection$filterDate(&quot;2013-01-01&quot;, &quot;2014-01-01&quot;) %&gt;% ee$ImageCollection$map(function(x) x$select(&quot;stable_lights&quot;)) %&gt;% ee$ImageCollection$toBands() Step 3: Download the information. ## Takes about 10 minutes lights_shape &lt;- map(unique(shape$dam2), ~tryCatch(ee_extract( x = lights, y = shape[&quot;dam2&quot;] %&gt;% filter(dam2 == .x), ee$Reducer$mean(), sf = FALSE ) %&gt;% mutate(dam2 = .x), error = function(e)data.frame(dam2 = .x))) lights_shape %&lt;&gt;% bind_rows() tba(lights_shape, cap = &quot;Average of night lights&quot;) Tabla 1.1: Average standardized night lights dam2 stable_lights_mean 0101 0.9393 0102 1.6857 0103 1.6900 0201 -0.4380 0202 1.4627 0203 1.3519 0204 1.6333 0205 1.7522 0206 1.7522 0207 1.7444 Repeat the routine for: Soil type: crops-coverfraction (Percentage of crop cover) and urban-coverfraction (Percentage of urban cover) available at https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_Landcover_100m_Proba-V-C3_Global#description Travel time to the nearest hospital or clinic (accessibility) and travel time to the nearest hospital or clinic using non-motorized transport (accessibility_walking_only) information available at https://developers.google.com/earth-engine/datasets/catalog/Oxford_MAP_accessibility_to_healthcare_2019 Human modification, considering human settlements, agriculture, transportation, mining, energy production, and electrical infrastructure. You can find satellite information at the following link: https://developers.google.com/earth-engine/datasets/catalog/CSP_HM_GlobalHumanModification#description Paso 4: Consolidate the information. Tabla 1.2: Standardized satellite predictors dam2 stable_lights_mean crops.coverfraction_mean urban.coverfraction_mean gHM_mean accessibility_mean accessibility_walking_only_mean stable_lights_sum crops.coverfraction_sum urban.coverfraction_sum gHM_sum accessibility_sum accessibility_walking_only_sum 0101 0.9393 -0.5459 0.4390 0.5741 -0.7760 -0.9315 -1.2660 -0.5849 -0.8078 -1.1991 -0.6242 -0.8780 0102 1.6857 -0.7090 2.2891 1.8346 -0.8897 -1.2588 -1.7964 -0.5947 -1.2224 -1.2993 -0.6272 -0.8873 0103 1.6900 -0.3571 2.0344 1.7510 -0.8684 -1.2055 -1.6990 -0.5880 -1.0667 -1.2842 -0.6269 -0.8866 0201 -0.4380 -0.0874 -0.6524 -0.6504 0.0531 0.0511 1.0737 -0.1234 -0.6327 0.0186 -0.2048 -0.2380 0202 1.4627 -0.6237 1.1018 0.8775 -0.8226 -1.0846 -0.8523 -0.5884 0.1016 -1.1468 -0.6231 -0.8757 0203 1.3519 -0.6402 0.9281 0.8771 -0.6780 -0.9356 -0.8100 -0.5891 0.0754 -1.1302 -0.6160 -0.8673 0204 1.6333 -0.5050 0.9165 1.0157 -0.8334 -1.1168 -0.6630 -0.5781 0.0671 -1.1229 -0.6232 -0.8762 0205 1.7522 -0.6844 2.3011 1.8174 -0.8888 -1.2573 -1.2927 -0.5937 -0.0489 -1.2119 -0.6266 -0.8856 0206 1.7522 -0.4289 2.1777 1.8272 -0.8968 -1.2779 -1.8138 -0.5914 -1.3125 -1.3046 -0.6273 -0.8875 0207 1.7444 -0.4662 1.7771 1.6966 -0.8322 -1.1072 -1.5815 -0.5898 -1.0882 -1.2650 -0.6264 -0.8850 1.4.2 Night Lights 1.4.3 Crop Cover 1.4.4 Urban Cover 1.4.5 Human Modification Human Modification Sum Human Modification Satellite 1.4.6 Average Travel Time to Hospital Average Travel Time to Hospital Sum Average Travel Time to Hospital Satellite 1.4.7 Average Travel Time to Hospital by Non-Motorized Vehicle Average Travel Time to Hospital by Non-Motorized Vehicle Sum Average Travel Time to Hospital by Non-Motorized Vehicle Satellite "],["population-and-housing-censuses.html", "1.5 Population and Housing Censuses", " 1.5 Population and Housing Censuses It’s necessary to define the variables for the country you want to work with. As a first step, access to the country’s census data is required. You can access it from the following link: https://redatam.org/en/microdata, where you’ll find a .zip file with the microdata for the country. To read this dataset, you’ll need to use the redatam.open function from the redatam library. This function directly depends on the census dictionary from REDATAM software, which is a file with a .dicx extension and should be located in the same folder as the data being read. This is how an object is created within R that merges the dictionary with the microdata from the census database. After performing a process in R using REDATAM syntax, we have the following table: dam2 area1 sex2 age tiene_sanitario tiene_electricidad tiene_acueducto tiene_gas eliminar_basura tiene_internet material_paredes material_techo TRANMODE_PRIVATE_CAR ODDJOB WORKED 0101 1.0000 0.5087 2.5043 0.0019 0.7596 0.9545 0.7728 0.7804 0.9453 0.0095 0.7589 0.1472 0.0090 0.3488 0102 1.0000 0.4754 2.4689 0.0011 0.9064 0.9867 0.9181 0.9084 0.9882 0.0007 0.9060 0.0680 0.0126 0.2859 0103 1.0000 0.5037 2.2858 0.0152 0.6930 0.9741 0.7440 0.7362 0.9712 0.0028 0.6942 0.0491 0.0135 0.2819 0201 0.5147 0.5060 2.5517 0.0138 0.2342 0.8546 0.2955 0.6589 0.8386 0.0159 0.2215 0.1709 0.0077 0.3647 0202 0.9986 0.5376 2.7635 0.0028 0.3852 0.8236 0.4958 0.4138 0.6884 0.0014 0.5081 0.4489 0.0046 0.4512 0203 0.9754 0.5432 2.8765 0.0015 0.3326 0.7915 0.4864 0.3495 0.5945 0.0014 0.5135 0.5314 0.0042 0.4880 0204 1.0000 0.5300 2.6401 0.0042 0.5720 0.8835 0.6198 0.6166 0.7998 0.0016 0.5975 0.3197 0.0071 0.4125 0205 1.0000 0.5182 2.6644 0.0013 0.8060 0.9590 0.8347 0.8130 0.9091 0.0030 0.8234 0.3291 0.0068 0.4559 0206 1.0000 0.5157 2.3750 0.0290 0.0285 0.8879 0.1433 0.1516 0.9034 0.0258 0.0320 0.0639 0.0139 0.2914 0207 1.0000 0.5097 2.4257 0.0465 0.1581 0.8925 0.2551 0.2337 0.9198 0.0162 0.1512 0.0717 0.0169 0.3121 1.5.1 Mapas de las variables con información censal. "],["session-2--generalized-variance-function.html", "Chapter 2 Session 2- Generalized Variance Function", " Chapter 2 Session 2- Generalized Variance Function One of the most important inputs in the area model is the variance of the direct estimator at the domain level, which cannot be calculated directly. Accordingly, this value must be estimated from the data collected in each domain. However, in domains with very small sample sizes, these estimations will not perform well. Hence, it is very useful to use a smoothing model for variances to eliminate noise and volatility from these estimations and extract the true signal of the process. Hidiroglou (2019) states that \\(E_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta}\\) and \\(V_{\\mathscr{MP}}\\left(\\hat{\\theta}^{dir}_d\\right)=\\sigma_{u}^2+\\tilde{\\sigma}^2_{d}\\), where the subscript \\(\\mathscr{MP}\\) refers to the double inference that must be taken into account in these adjustments and defines the joint probability measure between the model and the sampling design. \\(\\mathscr{M}\\) refers to the probability measure induced by modeling and the inclusion of auxiliary covariates (\\(\\boldsymbol{x}_{d}\\)). \\(\\mathscr{MP}\\) refers to the probability measure induced by the complex sampling design that yields direct estimations. The solution proposed here is known as the Generalized Variance Function, which involves fitting a log-linear model to the estimated direct variance. Starting from the fact that an unbiased estimator of \\(\\sigma^2\\) denoted by \\(\\hat{\\sigma}^2\\) is available, it follows that: \\[ E_{\\mathscr{MP}}\\left(\\hat{\\sigma}_{d}^{2}\\right)=E_{\\mathscr{M}}\\left(E_{\\mathscr{P}}\\left(\\hat{\\sigma}_{d}^{2}\\right)\\right)=E_{\\mathscr{M}}\\left(\\sigma_{d}^{2}\\right)=\\tilde{\\sigma}_{d}^{2} \\] The above equality can be interpreted as an unbiased and simple estimator of \\(\\tilde{\\sigma}_{d}^{2}\\), denoted as \\(\\hat{\\sigma}_{d}^{2}\\). However, this sampling estimator is unstable when the sample size is small, which is precisely the dominant paradigm in small area estimation. Rivest and Belmonte (2000) consider smoothing models for the estimation of direct variances defined as follows: \\[ \\log\\left(\\hat{\\sigma}_{d}^{2}\\right)=\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}+\\boldsymbol{\\varepsilon}_{d} \\] Where \\(\\boldsymbol{z}_{d}\\) is a vector of explanatory covariates that are functions of \\(\\boldsymbol{x}_{d}\\), \\(\\boldsymbol{\\alpha}\\) is a vector of parameters to be estimated, \\(\\boldsymbol{\\varepsilon}_{d}\\) are random errors with zero mean and constant variance, assumed to be identically distributed conditionally on \\(\\boldsymbol{z}_{d}\\). From the above model, the smoothed estimation of the sampling variance is given by: \\[ \\tilde{\\sigma}_{d}^{2}=E_{\\mathscr{MP}}\\left(\\sigma_{d}^{2}\\right)=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)\\times\\Delta \\] Where \\(E_{\\mathscr{MP}}\\left(\\varepsilon_{d}\\right)=\\Delta\\). There’s no need to specify a parametric distribution for the errors of this model. Using the method of moments, the following unbiased estimator for \\(\\Delta\\) is obtained: \\[ \\hat{\\Delta}=\\frac{\\sum_{d=1}^{D}\\hat{\\sigma}_{d}^{2}}{\\sum_{d=1}^{D}\\exp\\left(\\boldsymbol{z}_{d}^{T}\\boldsymbol{\\alpha}\\right)} \\] Similarly, using standard procedures in linear regression, the estimation of the regression parameter coefficients is given by the following expression: \\[ \\hat{\\boldsymbol{\\alpha}}=\\left(\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\boldsymbol{z}_{d}^{T}\\right)^{-1}\\sum_{d=1}^{D}\\boldsymbol{z}_{d}\\log\\left(\\hat{\\sigma}_{d}^{2}\\right) \\] Finally, the smoothed estimator of the sampling variance is defined as: \\[ \\hat{\\tilde{\\sigma}}_{d}^{2}=\\exp\\left(\\boldsymbol{z}_{d}^{T}\\hat{\\boldsymbol{\\alpha}}\\right)\\hat{\\Delta} \\] Survey Data: The following code processes data using various R packages such as survey, tidyverse, srvyr, TeachingSampling, and haven. Library Loading: The code loads the necessary libraries (survey, tidyverse, srvyr, TeachingSampling, haven) required for data manipulation and analysis. Survey Data Set Reading: The code reads the dataset named ‘data_JAM.rds’ using the readRDS function. Data Manipulation: It creates new variables (dam2, fep, upm, estrato, ingreso, pobreza) using the mutate function from the dplyr package. Filters the dataset based on a specific condition using the filter function. library(survey) library(tidyverse) library(srvyr) library(TeachingSampling) library(haven) #read in data set # Q518A: Gross average income From Employment encuesta &lt;- read_rds(&quot;Recursos/02_FGV/01_data_JAM.rds&quot;) encuesta &lt;- encuesta %&gt;% mutate( dam2, fep = RFACT/4, upm = paste0(PAR_COD , CONST_NUMBER, ED_NUMBER), estrato = ifelse(is.na(STRATA) ,strata,STRATA), ingreso = case_when(!is.na(Q518A) &amp; !is.na(Q518B) ~ Q518A + Q518B, !is.na(Q518A) &amp; !is.na(Q518B) ~ NA_real_, !is.na(Q518A) &amp; is.na(Q518B) ~ Q518A, is.na(Q518A) &amp; !is.na(Q518B) ~ Q518B), pobreza = ifelse(ingreso &lt; 50,1,0) ) %&gt;% filter(ingreso &gt; 11) dam2: Corresponds to the code assigned to the country’s second administrative division. The income definition is structured in this manner to illustrate the process utilized in the small area estimation methodology for poverty estimation. ID QUARTER EMPSTATUS STRATA CLUSTER CONST_NUMBER ED_NUMBER URCODE SEX AGE Q518A Q518AP Q518B Q518BP RFACT PAR_COD PARISH strata cluster dam2 fep upm estrato ingreso pobreza 02010105902170101 3 5 014 014011 01 059 1 2 44 9 NA 9e+00 NA 127.4657 01 Kingston NA NA 0101 31.8664 0101059 014 18 1 02020207900130103 3 5 235 235472 02 079 2 1 36 0 NA 1e+05 4 164.6809 12 Manchester NA NA 1202 41.1702 1202079 235 100000 0 02020404901820105 3 5 239 239488 04 049 2 1 25 NA NA 1e+04 3 152.1773 12 Manchester NA NA 1204 38.0443 1204049 239 10000 0 04010400700370102 3 5 036 036073 04 007 1 2 34 9 NA 9e+00 NA 188.7227 02 St Andrew NA NA 0204 47.1807 0204007 036 18 1 04030406400160106 3 5 221 221453 04 064 3 1 20 15000 1 0e+00 NA 308.0567 11 St Elizabeth NA NA 1104 77.0142 1104064 221 15000 0 05020506600550101 3 5 164 164352 05 066 2 2 60 9 NA 9e+00 NA 94.0227 08 St James NA NA 0805 23.5057 0805066 164 18 1 06030106300920101 3 5 122 122227 01 063 3 1 44 9 NA 9e+00 NA 252.1380 05 St Mary NA NA 0501 63.0345 0501063 122 18 1 06030200300140102 3 5 228 228464 02 003 3 2 70 9 NA 9e+00 NA 49.1197 12 Manchester NA NA 1202 12.2799 1202003 228 18 1 06030204100960101 3 5 084 084180 02 041 3 2 54 9 NA 9e+00 NA 97.2597 03 St Thomas NA NA 0302 24.3149 0302041 084 18 1 06030208501550102 3 5 234 234474 02 085 3 2 77 0 NA 3e+04 2 70.9262 12 Manchester NA NA 1202 17.7315 1202085 234 30000 0 In the following code block, the libraries survey and srvyr are used to create a sampling design from a survey database. The sampling design encompasses information about primary sampling units (PSUs), sampling weights (wkx), and strata (estrato) utilized in the sampling. Additionally, the “survey.lonely.psu” option is employed to adjust sample sizes within groups of primary sampling units that lack other primary sampling units within the same group. library(survey) library(srvyr) options(survey.lonely.psu = &quot;adjust&quot;) id_dominio &lt;- &quot;dam2&quot; diseno &lt;- as_survey_design( ids = upm, weights = fep, strata = estrato, nest = TRUE, .data = encuesta ) #summary(diseno) ´ Indicator Calculation: Groups the sampling design by domain ID and calculates indicators related to poverty (n_pobreza and pobreza). n_pobreza counts the number of instances where pobreza equals 1 (indicating poverty), while pobreza computes the survey mean of the pobreza variable with specific variance estimations. # Calculating indicators related to poverty and counts of UPMS per domain # Indicator Calculation: # Grouping the sampling design by domain ID and summarizing variables related to poverty. indicador_dam &lt;- diseno %&gt;% group_by_at(id_dominio) %&gt;% summarise( n_pobreza = unweighted(sum(pobreza == 1)), pobreza = survey_mean(pobreza, vartype = c(&quot;se&quot;, &quot;var&quot;), deff = T ) ) Counts of UPMS per Domain: Extracts domain ID and UPMs from the survey dataset, obtaining unique UPMs per domain and counting them. Joins the count of unique UPMs per domain with the previously calculated indicators related to poverty. # Counts of UPMS per domain: # Selecting domain ID and UPMs, obtaining unique UPMs per domain, and counting them. # Joining the count of unique UPMs per domain with previously calculated indicators related to poverty. indicador_dam &lt;- encuesta %&gt;% select(id_dominio, upm) %&gt;% distinct() %&gt;% group_by_at(id_dominio) %&gt;% tally(name = &quot;n_upm&quot;) %&gt;% inner_join(indicador_dam, by = id_dominio) # saveRDS(directodam2, &quot;Recursos/02_FGV/indicador_dam.Rds&quot;) dam2 n_upm n_pobreza pobreza pobreza_se pobreza_var pobreza_deff 0101 17 237 0.9980 0.0021 0.0000 5.228000e-01 0102 12 197 0.9836 0.0089 0.0001 1.053700e+00 0103 9 65 1.0000 0.0000 0.0000 3.065785e+32 0201 11 244 1.0000 0.0000 0.0000 NaN 0202 11 141 0.9391 0.0292 0.0009 2.317700e+00 0203 12 101 0.8117 0.0775 0.0060 4.795200e+00 0204 11 224 1.0000 0.0000 0.0000 NaN 0205 9 85 0.9646 0.0331 0.0011 2.932600e+00 0206 8 102 1.0000 0.0000 0.0000 NaN 0207 11 149 1.0000 0.0000 0.0000 NaN Ahora se realiza el filtro de los dominios con 5 o más UPMs y todos los que tengan un deff mayor que 1 base_sae &lt;- indicador_dam %&gt;% filter(n_upm &gt;= 5, pobreza_deff &gt; 1) seguidamente se realiza la transformación \\(\\log(\\hat{\\sigma}^2_d)\\), además se realiza la selección de las columnas identificador del municipio (dam2), la estimación directa (pobreza), El número de personas en el dominio (nd) y la varianza estimada del para la estimación directa vardir,siendo esta la que transforma mediante la función log(). baseFGV &lt;- base_sae %&gt;% select(dam2, pobreza, nd = n_pobreza, vardir = pobreza_var) %&gt;% mutate(ln_sigma2 = log(vardir)) "],["graphical-analysis.html", "2.1 Graphical Analysis", " 2.1 Graphical Analysis The first graph, p1, displays a scatter plot of the variable ln_sigma2 against the variable pobreza, with a smooth line representing a trend estimation. The x-axis is labeled as pobreza. The second graph, p2, exhibits a scatter plot of the variable ln_sigma2 against the variable nd, with a smooth line indicating a trend estimation. The x-axis is labeled as Tamaño de muestra (Sample Size). The third graph, p3, demonstrates a scatter plot of the variable ln_sigma2 in relation to the product of pobreza and nd, with a smooth line representing a trend estimation. The x-axis is labeled as Número de pobres (Number of Poor). The fourth graph, p4, shows a scatter plot of the variable ln_sigma2 against the square root of the variable pobreza, with a smooth line representing a trend estimation. The x-axis is labeled as Raiz cuadrada de pobreza (Square Root of Poverty). Overall, these graphs are designed to explore the relationship between ln_sigma2 and different independent variables such as pobreza, nd, and the square root of poverty. Choosing to use the “loess” function to smooth the lines instead of a straight line aids in visualizing general trends in the data more effectively. theme_set(theme_bw()) # pobreza vs Ln_sigma2 # p1 &lt;- ggplot(baseFGV, aes(x = pobreza, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;poverty&quot;) # Sample Size vs Ln_sigma2 # p2 &lt;- ggplot(baseFGV, aes(x = nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Sample size&quot;) # Number of Poor vs Ln_sigma2 # p3 &lt;- ggplot(baseFGV, aes(x = pobreza * nd, y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Number of poor&quot;) # Square Root of Poverty vs Ln_sigma2 # p4 &lt;- ggplot(baseFGV, aes(x = sqrt(pobreza), y = ln_sigma2)) + geom_point() + geom_smooth(method = &quot;loess&quot;) + xlab(&quot;Square root of poverty&quot;) library(patchwork) (p1 | p2) / (p3 | p4) "],["variance-model.html", "2.2 Variance Model", " 2.2 Variance Model The code fits a multiple linear regression model (using the lm() function), where ln_sigma2 is the response variable and the predictor variables include pobreza, nd, and various transformations of these variables. The goal of this model is to estimate the generalized variance function (FGV) for the observed domains. library(gtsummary) FGV1 &lt;- lm(ln_sigma2 ~ -1 + pobreza + I(pobreza*nd), data = baseFGV) tbl_regression(FGV1) %&gt;% add_glance_table(include = c(r.squared, adj.r.squared)) #venishuuyv table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #venishuuyv thead, #venishuuyv tbody, #venishuuyv tfoot, #venishuuyv tr, #venishuuyv td, #venishuuyv th { border-style: none; } #venishuuyv p { margin: 0; padding: 0; } #venishuuyv .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #venishuuyv .gt_caption { padding-top: 4px; padding-bottom: 4px; } #venishuuyv .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #venishuuyv .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #venishuuyv .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #venishuuyv .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #venishuuyv .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #venishuuyv .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #venishuuyv .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #venishuuyv .gt_column_spanner_outer:first-child { padding-left: 0; } #venishuuyv .gt_column_spanner_outer:last-child { padding-right: 0; } #venishuuyv .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #venishuuyv .gt_spanner_row { border-bottom-style: hidden; } #venishuuyv .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #venishuuyv .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #venishuuyv .gt_from_md > :first-child { margin-top: 0; } #venishuuyv .gt_from_md > :last-child { margin-bottom: 0; } #venishuuyv .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #venishuuyv .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #venishuuyv .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #venishuuyv .gt_row_group_first td { border-top-width: 2px; } #venishuuyv .gt_row_group_first th { border-top-width: 2px; } #venishuuyv .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #venishuuyv .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #venishuuyv .gt_first_summary_row.thick { border-top-width: 2px; } #venishuuyv .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #venishuuyv .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #venishuuyv .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #venishuuyv .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #venishuuyv .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #venishuuyv .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #venishuuyv .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #venishuuyv .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #venishuuyv .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #venishuuyv .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #venishuuyv .gt_left { text-align: left; } #venishuuyv .gt_center { text-align: center; } #venishuuyv .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #venishuuyv .gt_font_normal { font-weight: normal; } #venishuuyv .gt_font_bold { font-weight: bold; } #venishuuyv .gt_font_italic { font-style: italic; } #venishuuyv .gt_super { font-size: 65%; } #venishuuyv .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #venishuuyv .gt_asterisk { font-size: 100%; vertical-align: 0; } #venishuuyv .gt_indent_1 { text-indent: 5px; } #venishuuyv .gt_indent_2 { text-indent: 10px; } #venishuuyv .gt_indent_3 { text-indent: 15px; } #venishuuyv .gt_indent_4 { text-indent: 20px; } #venishuuyv .gt_indent_5 { text-indent: 25px; } Characteristic Beta 95% CI1 p-value pobreza -12 -20, -4.5 0.003 I(pobreza * nd) 0.02 -0.04, 0.07 0.5 R² 0.345 Adjusted R² 0.311 1 CI = Confidence Interval After obtaining the model estimation, the value of the constant \\(\\Delta\\) must be obtained, for which the following code is used. delta.hat = sum(baseFGV$vardir) / sum(exp(fitted.values(FGV1))) From which it is derived that \\(\\Delta = 0.110434\\). Finally, it is possible to obtain the smoothed variance by executing the following command. hat.sigma &lt;- data.frame(dam2 = baseFGV$dam2, hat_var = delta.hat * exp(fitted.values(FGV1))) baseFGV &lt;- left_join(baseFGV, hat.sigma) tba(head(baseFGV, 10)) dam2 pobreza nd vardir ln_sigma2 hat_var 0102 0.9836 197 0.0001 -9.4372 0e+00 0103 1.0000 65 0.0000 -76.3620 0e+00 0202 0.9391 141 0.0009 -7.0701 0e+00 0203 0.8117 101 0.0060 -5.1159 0e+00 0205 0.9646 85 0.0011 -6.8149 0e+00 0212 0.8304 59 0.0101 -4.5936 0e+00 0301 0.9419 45 0.0013 -6.6569 0e+00 0302 0.9109 159 0.0017 -6.3681 0e+00 0401 0.8063 319 0.0006 -7.4369 4e-04 0402 0.8311 259 0.0012 -6.7172 1e-04 Model validation for the FGV par(mfrow = c(2, 2)) plot(FGV1) Smoothed variance prediction base_sae &lt;- left_join(indicador_dam, baseFGV %&gt;% select(id_dominio, hat_var), by = id_dominio) %&gt;% mutate( pobreza_var = ifelse(is.na(hat_var), NA_real_, pobreza_var), pobreza_deff = ifelse(is.na(hat_var), NA_real_, pobreza_deff) ) Now, we make a line graph to see the volatility and the estimates of the variances. nDom &lt;- sum(!is.na(base_sae$hat_var)) temp_FH &lt;- base_sae %&gt;% filter(!is.na(hat_var)) ggplot(temp_FH %&gt;% arrange(n_pobreza), aes(x = 1:nDom)) + geom_line(aes(y = pobreza_var, color = &quot;VarDirEst&quot;)) + geom_line(aes(y = hat_var, color = &quot;FGV&quot;)) + labs(y = &quot;Varianzas&quot;, x = &quot;Sample size&quot;, color = &quot; &quot;) + scale_x_continuous(breaks = seq(1, nDom, by = 10), labels = temp_FH$n_pobreza[order(temp_FH$n_pobreza)][seq(1, nDom, by = 10)]) + scale_color_manual(values = c(&quot;FGV&quot; = &quot;Blue&quot;, &quot;VarDirEst&quot; = &quot;Red&quot;)) This code performs several transformations on the dataset base_sae: Creation of new variables: pobreza_deff: Replaces NaN values with 1 if they exist; otherwise, it keeps the original value. deff_FGV: Computes a new Design Effect (DEFF) using the formula hat_var / (pobreza_var / pobreza_deff) when pobreza_var is not equal to 0. n_eff_FGV: Calculates the effective number of surveyed individuals as n_pobreza / deff_FGV. Modification of the variable pobreza: If hat_var is NA, it replaces pobreza values with NA; otherwise, it retains the original value. base_FH &lt;- base_sae %&gt;% mutate( pobreza_deff = ifelse(is.nan(pobreza_deff), 1, pobreza_deff), deff_FGV = ifelse(pobreza_var == 0 , 1, hat_var / (pobreza_var / pobreza_deff) #Fórmula del nuevo DEFF ), # Criterio MDS para regularizar el DeffFGV n_eff_FGV = n_pobreza / deff_FGV, #Número efectivo de personas encuestadas pobreza = ifelse(is.na(hat_var), NA_real_, pobreza) ) #saveRDS(object = base_FH, &quot;Recursos/02_FGV/base_FH_2020.rds&quot;) "],["session-1--fay-herriot-model---poverty-estimation.html", "Chapter 3 Session 1- Fay Herriot Model - Poverty Estimation", " Chapter 3 Session 1- Fay Herriot Model - Poverty Estimation The Fay Herriot model, proposed by Fay and Herriot (1979), is a statistical area model and is the most commonly used. It’s essential to note that within small area estimation methodology, area models are the most applied because having individual-level information is often not feasible. Instead, we typically have data at the area level along with associated auxiliary information. This mixed linear model was the first to include random effects at the area level, implying that most of the information fed into the model corresponds to usually aggregated units like departments, regions, provinces, municipalities, among others. The estimates obtained from the model are over these aggregations or subpopulations. Now, the Fay Herriot model relates area indicators \\(\\theta_d\\), where \\(d\\) ranges from 1 to \\(D\\), assuming they vary with respect to a vector of \\(p\\) covariates \\(\\boldsymbol{x}_d\\). The model is defined by the equation \\(\\theta_d = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d\\), where \\(u_d\\) is the error term or random effect, distinct for each area and distributed as \\(u_{d} \\stackrel{ind}{\\sim}\\left(0,\\sigma_{u}^{2}\\right)\\). However, the true values of the indicators \\(\\theta_d\\) are not observable. Hence, the direct estimator \\(\\hat{\\theta}^{DIR}_d\\) is used to estimate them, leading to sampling error. This estimator is still considered unbiased under the sample design, i.e., \\[ \\hat{\\theta}_d^{DIR} = \\theta + e_d \\] The model is then fitted using the sampling error term \\(e_d\\), where \\(e_{d} \\stackrel{ind}{\\sim} \\left(0,\\sigma^2_{e_d}\\right)\\), and the variances \\(\\sigma^2_{e_d}\\) are estimated using survey microdata. The FH model is rewritten as \\[ \\hat{\\theta}^{DIR}_{d} = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d + e_d \\]. The best linear unbiased predictor (BLUP) under the FH model is given by \\[ \\tilde{\\theta}_{d}^{FH} = \\boldsymbol{x}^{T}_{d}\\tilde{\\boldsymbol{\\beta}}+\\tilde{u}_{d} \\], where \\(\\tilde{u}_d = \\gamma_d\\left(\\hat{\\theta}^{DIR}_{d} - \\boldsymbol{x}^{T}_{d}\\tilde{\\boldsymbol{\\beta}} \\right)\\) and \\(\\gamma_d=\\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_{e_d}}\\). Area Model for Poverty Estimation Let \\(P_d\\) be the probability of finding a person in a state of poverty in the \\(d\\)-th domain of the population. Then, the direct estimator of \\(P_d\\) can be written as: \\[ \\hat{P}^{DIR}_{d} = P_d + e_d \\] Now, \\(P_d\\) can be modeled as follows: \\[ P_d = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d \\] Rewriting \\(\\hat{P}^{DIR}_{d}\\) in terms of the two previous equations, we have: \\[ \\hat{P}^{DIR}_{d} = \\boldsymbol{x}^{T}_{d}\\boldsymbol{\\beta} + u_d + e_d \\] It is then possible to assume that \\(\\hat{P}^{DIR}_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta, \\sigma_u^2 +\\sigma_{e_d}^2)\\), \\(\\hat{P}^{DIR}_d \\mid u_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + u_d,\\sigma_{e_d}^2)\\), and \\(u_d \\sim N(0, \\sigma^2_u)\\). Next, prior distributions are assumed for \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2_u\\): \\[ \\begin{align*} \\beta_p &amp; \\sim N(0, 10000)\\\\ \\sigma^2_u &amp;\\sim IG(0.0001, 0.0001) \\end{align*} \\] Therefore, the Bayesian estimator for \\(P_d\\) is given as \\(\\tilde{P}_d = E\\left(P_d\\mid\\hat{P}_d^{DIR}\\right)\\). Optimal Predictor of \\(P_d\\) The optimal predictor of \\(P_d\\) is given by: \\[E(P_d | \\hat{P}^{DIR}_d) = \\gamma_d\\hat{P}^{DIR}_d + (1-\\gamma_d)\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta\\] where \\(\\gamma_d = \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\). We know that \\(\\hat{P}^{DIR}_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta, \\sigma_u^2 +\\sigma_{e_d}^2)\\), \\(\\hat{P}^{DIR}_d \\mid u_d \\sim N(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + u_d,\\sigma_{e_d}^2)\\), and \\(u_d \\sim N(0, \\sigma^2_u)\\). Therefore, the optimal predictor is determined by a weighted combination of the direct estimator \\(\\hat{P}^{DIR}_d\\) and the linear predictor \\(\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta\\) where the weights are determined by the ratio of the variance components, providing an optimal balance between the direct estimate and the model-based prediction. \\[ \\begin{align*} f(u_d| \\hat{P}^{DIR}_d) \\propto f(\\hat{P}^{DIR}_d | u_d)f(u_d) &amp; = \\frac{1}{\\sigma^2_{e_d}\\sqrt{2\\pi}}\\exp\\left\\{-\\frac{1}{2\\sigma^2_{e_d}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta - u_d)^2}\\right\\} \\frac{1}{\\sigma^2_u\\sqrt{2\\pi}}\\exp\\left\\{- \\frac{1}{2\\sigma^2_u}u_d^2\\right\\}\\\\ &amp; \\propto \\exp\\left\\{-\\frac{u_d^2 - 2u_d(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)}{2\\sigma^2_{e_d}} - \\frac{u_d^2}{2\\sigma^2_u}\\right\\} \\\\ &amp; = \\exp\\left\\{-\\frac{1}{2}\\left[(\\frac{1}{\\sigma^2_{e_d}} + \\frac{1}{\\sigma^2_u})u_d^2 - 2\\frac{\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta}{\\sigma_{e_d}^2}u_d\\right] \\right\\} \\\\ &amp; = \\exp \\left\\{ -\\frac{1}{2\\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}}\\left[u_d^2 - 2\\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)u_d \\right] \\right\\} \\\\ &amp; \\propto \\exp \\left\\{ -\\frac{1}{2\\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}}\\left[u_d - \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)\\right]^2 \\right\\} \\\\ &amp; \\propto N(E(u_d|\\hat{P}^{DIR}_d), \\text{Var}(u_d|P^{DIR})) \\end{align*} \\] with \\(E(u_d|\\hat{P}^{DIR}_d) = \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta)\\) y \\(\\text{Var}(u_d|P^{DIR}) = \\frac{\\sigma_u^2\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\). Therefore you have, \\[ \\begin{align*} E(P_d | \\hat{P}^{DIR}_d) = \\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + E(u_d|\\hat{P}^{DIR}_d) &amp; = \\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta + \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}(\\hat{P}^{DIR}_d-\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta) \\\\ &amp; = \\frac{\\sigma_{e_d}^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\hat{P}^{DIR}_d + \\frac{\\sigma_u^2}{\\sigma_u^2 +\\sigma_{e_d}^2}\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta \\\\ &amp; = \\gamma_d\\hat{P}^{DIR}_d + (1-\\gamma_d)\\boldsymbol{x}^{T}_{d}\\boldsymbol \\beta \\end{align*} \\] "],["estimation-procedure.html", "3.1 Estimation Procedure", " 3.1 Estimation Procedure This code utilizes the libraries tidyverse and magrittr for data processing and analysis. The function readRDS() is used to load a data file in RDS format, containing direct estimates and smoothed variance for the proportion of individuals in poverty for the year 2018. Subsequently, the %&gt;% operator from the magrittr library is employed to chain the selection of specific columns, namely dam2, nd, pobreza, vardir, and hat_var. library(tidyverse) library(magrittr) base_FH &lt;- readRDS(&quot;Recursos/03_FH_normal/01_base_FH.Rds&quot;) %&gt;% select(dam2,pobreza,hat_var) Reading the covariates, which have been previously obtained. Due to the difference in scales between variables, an adjustment is necessary. statelevel_predictors_df &lt;- readRDS(&#39;Recursos/03_FH_normal/02_statelevel_predictors_dam.rds&#39;) %&gt;% mutate(id_order = 1:n()) Next, a full join (full_join) is performed between the dataset base_FH and the predictors statelevel_predictors_df using the variable dam2 as the joining key. The function tba() is used to display the first 10 rows and 8 columns of the resulting dataset from the previous join. A full join (full_join) combines data from both datasets, preserving all rows from both and filling in missing values (NA) if matches aren’t found based on the joining variable (dam2 in this case). The tba() function displays an HTML-formatted table in the R console showing the first 10 rows and 8 columns of the resulting dataset from the join. base_FH &lt;- full_join(base_FH, statelevel_predictors_df, by = &quot;dam2&quot; ) tba(base_FH[1:10,1:8]) dam2 pobreza hat_var area1 sex2 age tiene_sanitario tiene_electricidad 0101 NA NA 1.0000 0.5087 2.5043 0.0019 0.7596 0102 0.9836 0 1.0000 0.4754 2.4689 0.0011 0.9064 0103 1.0000 0 1.0000 0.5037 2.2858 0.0152 0.6930 0201 NA NA 0.5147 0.5060 2.5517 0.0138 0.2342 0202 0.9391 0 0.9986 0.5376 2.7635 0.0028 0.3852 0203 0.8117 0 0.9754 0.5432 2.8765 0.0015 0.3326 0204 NA NA 1.0000 0.5300 2.6401 0.0042 0.5720 0205 0.9646 0 1.0000 0.5182 2.6644 0.0013 0.8060 0206 NA NA 1.0000 0.5157 2.3750 0.0290 0.0285 0207 NA NA 1.0000 0.5097 2.4257 0.0465 0.1581 # View(base_FH) "],["preparing-the-supplies-for-stan.html", "3.2 Preparing the supplies for STAN", " 3.2 Preparing the supplies for STAN Splitting the database into observed and unobserved domains. Observed domains. data_dir &lt;- base_FH %&gt;% filter(!is.na(pobreza)) Unobserved domains. data_syn &lt;- base_FH %&gt;% anti_join(data_dir %&gt;% select(dam2)) tba(data_syn[1:10, 1:8]) dam2 pobreza hat_var area1 sex2 age tiene_sanitario tiene_electricidad 0101 NA NA 1.0000 0.5087 2.5043 0.0019 0.7596 0201 NA NA 0.5147 0.5060 2.5517 0.0138 0.2342 0204 NA NA 1.0000 0.5300 2.6401 0.0042 0.5720 0206 NA NA 1.0000 0.5157 2.3750 0.0290 0.0285 0207 NA NA 1.0000 0.5097 2.4257 0.0465 0.1581 0208 NA NA 1.0000 0.5256 2.6193 0.0018 0.4508 0209 NA NA 1.0000 0.5149 2.4776 0.0066 0.3041 0210 NA NA 0.9779 0.5194 2.5153 0.0144 0.2003 0211 NA NA 1.0000 0.5427 2.8148 0.0006 0.5348 0502 NA NA 0.3699 0.4974 2.5265 0.0127 0.3983 Defining the fixed-effects matrix. Defines a linear model using the formula() function, incorporating various predictor variables such as age, ethnicity, unemployment rate, among others. Utilizes the model.matrix() function to generate design matrices (Xdat and Xs) from the observed (data_observed) and unobserved (data_unobserved) data to use in building regression models. The model.matrix() function transforms categorical variables into binary (dummy) variables, allowing them to be used in modeling. formula_mod &lt;- formula(~ ODDJOB + WORKED + stable_lights_mean + accessibility_mean + urban.coverfraction_sum) ## Dominios observados Xdat &lt;- model.matrix(formula_mod, data = data_dir) ## Dominios no observados Xs &lt;- model.matrix(formula_mod, data = data_syn) dim(Xs) ## [1] 22 6 dim(data_syn) ## [1] 22 30 Creando lista de parámetros para STAN sample_data &lt;- list( N1 = nrow(Xdat), # Observed. N2 = nrow(Xs), # Not observed. p = ncol(Xdat), # Number of predictors. X = as.matrix(Xdat), # Observed covariates. Xs = as.matrix(Xs), # Not observed covariates. y = as.numeric(data_dir$pobreza), # Direct estimation sigma_e = sqrt(data_dir$hat_var) # Estimation error ) Rutina implementada en STAN data { int&lt;lower=0&gt; N1; // number of data items int&lt;lower=0&gt; N2; // number of data items for prediction int&lt;lower=0&gt; p; // number of predictors matrix[N1, p] X; // predictor matrix matrix[N2, p] Xs; // predictor matrix vector[N1] y; // predictor matrix vector[N1] sigma_e; // known variances } parameters { vector[p] beta; // coefficients for predictors real&lt;lower=0&gt; sigma2_u; vector[N1] u; } transformed parameters{ vector[N1] theta; vector[N1] thetaSyn; vector[N1] thetaFH; vector[N1] gammaj; real&lt;lower=0&gt; sigma_u; thetaSyn = X * beta; theta = thetaSyn + u; sigma_u = sqrt(sigma2_u); gammaj = to_vector(sigma_u ./ (sigma_u + sigma_e)); thetaFH = (gammaj) .* y + (1-gammaj).*thetaSyn; } model { // likelihood y ~ normal(theta, sigma_e); // priors beta ~ normal(0, 100); u ~ normal(0, sigma_u); sigma2_u ~ inv_gamma(0.0001, 0.0001); } generated quantities{ vector[N2] y_pred; for(j in 1:N2) { y_pred[j] = normal_rng(Xs[j] * beta, sigma_u); } } Compiling the model in STAN. Here’s the process to compile the STAN code from R: This code utilizes the rstan library to fit a Bayesian model using the file 17FH_normal.stan, which contains the model written in the Stan probabilistic modeling language. Initially, the stan() function is employed to fit the model to the sample_data. The arguments passed to stan() include the file containing the model (fit_FH_normal), the data (sample_data), and control arguments for managing the model fitting process, such as the number of iterations for the warmup period (warmup), the sampling period (iter), and the number of CPU cores to use for the fitting process (cores). Additionally, the parallel::detectCores() function is used to automatically detect the number of available CPU cores. The mc.cores option is then set to utilize the maximum number of available cores for the model fitting. The outcome of the model fitting is stored in model_FH_normal, which contains a sample from the posterior distribution of the model. This sample can be employed for inferences about the model parameters and predictions. Overall, this code is useful for fitting Bayesian models using Stan and conducting subsequent inferences. library(rstan) fit_FH_normal &lt;- &quot;Recursos/03_FH_normal/modelosStan/17FH_normal.stan&quot; options(mc.cores = parallel::detectCores()) rstan::rstan_options(auto_write = TRUE) # speed up running time model_FH_normal &lt;- stan( file = fit_FH_normal, data = sample_data, verbose = FALSE, warmup = 2500, iter = 3000, cores = 4 ) saveRDS(object = model_FH_normal, file = &quot;Recursos/03_FH_normal/03_model_FH_normal.rds&quot;) Leer el modelo model_FH_normal&lt;- readRDS(&quot;Recursos/03_FH_normal/03_model_FH_normal.rds&quot;) 3.2.1 Results of the model for observed domains. In this code, the bayesplot, posterior, and patchwork libraries are loaded to create graphics and visualizations of the model results. Subsequently, the as.array() and as_draws_matrix() functions are used to extract samples from the posterior distribution of the parameter theta from the model. Then, 100 rows of these samples are randomly selected using the sample() function, resulting in the y_pred2 matrix. Finally, the ppc_dens_overlay() function from bayesplot is utilized to plot a comparison between the empirical distribution of the observed variable pobreza in the data (data_dir$pobreza) and the simulated posterior predictive distributions for the same variable (y_pred2). The ppc_dens_overlay() function generates a density plot for both distributions, facilitating the visualization of their comparison. library(bayesplot) library(posterior) library(patchwork) y_pred_B &lt;- as.array(model_FH_normal, pars = &quot;theta&quot;) %&gt;% as_draws_matrix() rowsrandom &lt;- sample(nrow(y_pred_B), 100) y_pred2 &lt;- y_pred_B[rowsrandom, ] p1 &lt;- ppc_dens_overlay(y = as.numeric(data_dir$pobreza), y_pred2) # ggsave(plot = p1, # filename = &quot;Recursos/Día2/Sesion1/0Recursos/FH1.png&quot;, # scale = 2) p1 + geom_vline(xintercept = 0, color = &quot;red&quot;) The results indicate that using the Fay-Herriot normal method is not possible, as we are obtaining outcomes outside the boundaries. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
